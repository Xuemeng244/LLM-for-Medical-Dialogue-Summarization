{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a44020e-a8c9-47d5-b77b-e20dd96e4944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac89ca69-7307-4180-888b-b470de1ceb49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.cuda.device_count()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9691403-d603-4c0b-8ae4-fbfaead8a086",
   "metadata": {},
   "source": [
    "## Step 1 导包 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b53dc897-d035-489c-8a15-676b235ef123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jie/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import *\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForLanguageModeling, Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "086f4e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MedicalDialogueDataset(Dataset):\n",
    "    def __init__(self, split, percent=100, seed=42):  # 添加seed参数，默认值为42\n",
    "        # 加载数据集\n",
    "        ds = load_dataset(\"omi-health/medical-dialogue-to-soap-summary\", split=split)\n",
    "        \n",
    "        # 移除不需要的列\n",
    "        columns_to_remove = ['messages', 'prompt']\n",
    "        ds = ds.remove_columns(columns_to_remove)\n",
    "        \n",
    "        # 替换换行符并重命名列\n",
    "        ds = ds.rename_column('soap', 'summary')\n",
    "        \n",
    "        # 添加ID和格式化摘要\n",
    "        ds = ds.map(self.add_id, with_indices=True)\n",
    "        ds = ds.map(self.format_summary)\n",
    "        \n",
    "        # 如果需要子集，先随机打乱，再选择对应的百分比\n",
    "        if percent < 100:\n",
    "            ds = ds.shuffle(seed=seed).select(range(int(percent / 100.0 * len(ds))))\n",
    "        \n",
    "        self.data = ds\n",
    "    \n",
    "    def add_id(self, example, idx):\n",
    "        example['id'] = str(idx)\n",
    "        return example\n",
    "    \n",
    "    def format_summary(self, example):\n",
    "        example['summary'] = example['summary'].replace('S: ', 'Subjective: ')\n",
    "        example['summary'] = example['summary'].replace('O: ', 'Objective: ')\n",
    "        example['summary'] = example['summary'].replace('A: ', 'Assessment: ')\n",
    "        example['summary'] = example['summary'].replace('P: ', 'Plan: ')\n",
    "        return example\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]  # 获取索引对应的数据项\n",
    "        ordered_item = {'id': item['id']}  # 创建一个新字典，并首先加入'id'\n",
    "        ordered_item.update({k: item[k] for k in item if k != 'id'})  # 添加其他字段，排除'id'\n",
    "        # return item['dialogue'], item['summary'] \n",
    "        return ordered_item\n",
    "\n",
    "# 创建不同百分比的训练数据集实例\n",
    "# train_data = MedicalDialogueDataset('train', percent=20, seed=42)\n",
    "train_data = MedicalDialogueDataset('train', percent=50, seed=42)\n",
    "# train_data = MedicalDialogueDataset('train', percent=100, seed=42)  \n",
    "\n",
    "valid_data = MedicalDialogueDataset('validation')\n",
    "test_data = MedicalDialogueDataset('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a743f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 示例初始化\n",
    "# train_data = MedicalDialogueDataset('train')\n",
    "# valid_data = MedicalDialogueDataset('validation')\n",
    "# test_data = MedicalDialogueDataset('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60c941c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 4625\n",
      "valid set size: 500\n",
      "test set size: 250\n",
      "{'id': '8647', 'dialogue': \"Doctor: Good morning, how can I help you today?\\nPatient: Hi doctor, I recently underwent an abdominal ultrasonography (USG) for my bilateral renal nephrolithiasis.\\nDoctor: I see. Tell me about your general health. How is your blood biochemistry, and do you have any cardiovascular or hormonal disorders?\\nPatient: My blood biochemistry is normal, and I don't have any cardiovascular or hormonal disorders. I had an operation 17 years ago to repair my extrophic bladder, and they created an Indiana pouch for me.\\nDoctor: Alright. Can you tell me about your weight and body mass index (BMI)?\\nPatient: My weight is 85 kg, and my BMI is 28.7 kg/m2.\\nDoctor: Thank you for the information. Now, let's talk about your USG results. It showed a hyperechogenic lesion at the fat intensity, filling out your right renal sinus completely. A computerized tomography (CT) scan confirmed the presence of a fatty mass that extended from the renal sinus down to the pelvis cuffing the right ureter throughout its entire length with severe hydronephrosis. Your left kidney seems unremarkable, except for a small scar at the upper pole. There's a suspicion for liposarcoma due to the presence of high-density regions inside the lesion. \\nPatient: Oh no, that sounds serious. What can be done about it?\\nDoctor: The best course of action was to perform a resection of the mass with right radical nephroureterectomy, which has already been done. The tumor was 16x13x6 cm in size, fatty in appearance, and marbled with irregular whitish solid areas or fibrosis. The entrapped ureter was stenotic proximally and distally, but dilated at its middle part. Your kidney was hydronephrotic with thinned out atrophic parenchyma, and a few small stones were detected inside the collecting system.\\nPatient: What did the microscopic examination of the tumor show?\\nDoctor: The microscopic examination showed mature fat tissue devoid of normal architecture, owing to expanded interstitial spaces either because of intense edema or irregular fibrous streaks. Fibrous septa between fat lobules contained mildly increased numbers of stromal fibroblasts, fine collagen, vascular proliferation, multifocal lymphocytic infiltration, occasional lymphoid follicles, and foamy histiocytes. Plasma cells were rare, and immunohistochemistry showed only a <2% IgG4+/ IgG+ plasma cell ratio. There were also scattered smooth muscle bundles, usually in close association with vessels. Ectatic branching lymphatic channels were not noted, and there were neither lipoblasts nor significant cellular atypia. A few scattered fibroblasts carried multilobulated large nuclei.\\nPatient: So, what does all of this mean for me?\\nDoctor: Your results indicate that the tumor has been successfully removed, and there are no signs of aggressive cancer. However, it is essential to monitor your condition and attend regular follow-up appointments to ensure everything remains stable.\\nPatient: Thank you, doctor. I'll make sure to schedule those follow-up appointments and keep an eye on my health.\\nDoctor: You're welcome. If you have any concerns or notice any changes in your health, please don't hesitate to contact me or schedule an appointment. Take care!\", 'summary': \"Subjective: The patient reported undergoing an abdominal ultrasonography for bilateral renal nephrolithiasis. He has a history of extrophic bladder repair with an Indiana pouch created 17 years ago. He denies any cardiovascular or hormonal disorders. His chief complaint relates to findings from recent imaging studies indicating a significant renal issue.\\nObjective: The patient's weight is 85 kg, and his BMI is 28.7 kg/m2. Blood biochemistry is within normal limits. Abdominal ultrasonography and CT scan revealed a hyperechogenic lesion at the fat intensity in the right renal sinus, extending to the pelvis and cuffing the right ureter, causing severe hydronephrosis. The left kidney shows a small scar but is otherwise unremarkable. A right radical nephroureterectomy was performed, removing a 16x13x6 cm fatty mass with features suggestive of liposarcoma. Microscopic examination showed mature fat tissue with expanded interstitial spaces, mild stromal fibroblast increase, and no significant cellular atypia or lipoblasts.\\nAssessment: The primary diagnosis is a benign fatty tumor of the right kidney with severe hydronephrosis and renal atrophy, likely secondary to the mass effect and chronic obstruction. Differential diagnosis included liposarcoma, which was ruled out based on histopathological findings. The prognosis is good following surgical resection, but regular monitoring is necessary to detect any recurrence or new complications.\\nPlan: The patient is advised to schedule regular follow-up appointments for monitoring his renal function and to detect any signs of recurrence early. He should maintain a healthy diet and hydration to support kidney function. Any new symptoms or changes in health should prompt an immediate consultation. Further consultations with a nephrologist and a urologist may be necessary to manage long-term outcomes and monitor renal function.\", 'messages_nosystem': [{'role': 'user', 'content': \"You are an expert medical professor assisting in the creation of medically accurate SOAP summaries. Please ensure the response follows the structured format: S:, O:, A:, P: without using markdown or special formatting. Create a Medical SOAP note summary from the dialogue, following these guidelines:\\n    S (Subjective): Summarize the patient's reported symptoms, including chief complaint and relevant history. Rely on the patient's statements as the primary source and ensure standardized terminology.\\n    O (Objective): Highlight critical findings such as vital signs, lab results, and imaging, emphasizing important details like the side of the body affected and specific dosages. Include normal ranges where relevant.\\n    A (Assessment): Offer a concise assessment combining subjective and objective data. State the primary diagnosis and any differential diagnoses, noting potential complications and the prognostic outlook.\\n    P (Plan): Outline the management plan, covering medication, diet, consultations, and education. Ensure to mention necessary referrals to other specialties and address compliance challenges.\\n    Considerations: Compile the report based solely on the transcript provided. Maintain confidentiality and document sensitively. Use concise medical jargon and abbreviations for effective doctor communication.\\n    Please format the summary in a clean, simple list format without using markdown or bullet points. Use 'S:', 'O:', 'A:', 'P:' directly followed by the text. Avoid any styling or special characters. ### Dialogue:\\nDoctor: Good morning, how can I help you today?\\nPatient: Hi doctor, I recently underwent an abdominal ultrasonography (USG) for my bilateral renal nephrolithiasis.\\nDoctor: I see. Tell me about your general health. How is your blood biochemistry, and do you have any cardiovascular or hormonal disorders?\\nPatient: My blood biochemistry is normal, and I don't have any cardiovascular or hormonal disorders. I had an operation 17 years ago to repair my extrophic bladder, and they created an Indiana pouch for me.\\nDoctor: Alright. Can you tell me about your weight and body mass index (BMI)?\\nPatient: My weight is 85 kg, and my BMI is 28.7 kg/m2.\\nDoctor: Thank you for the information. Now, let's talk about your USG results. It showed a hyperechogenic lesion at the fat intensity, filling out your right renal sinus completely. A computerized tomography (CT) scan confirmed the presence of a fatty mass that extended from the renal sinus down to the pelvis cuffing the right ureter throughout its entire length with severe hydronephrosis. Your left kidney seems unremarkable, except for a small scar at the upper pole. There's a suspicion for liposarcoma due to the presence of high-density regions inside the lesion. \\nPatient: Oh no, that sounds serious. What can be done about it?\\nDoctor: The best course of action was to perform a resection of the mass with right radical nephroureterectomy, which has already been done. The tumor was 16x13x6 cm in size, fatty in appearance, and marbled with irregular whitish solid areas or fibrosis. The entrapped ureter was stenotic proximally and distally, but dilated at its middle part. Your kidney was hydronephrotic with thinned out atrophic parenchyma, and a few small stones were detected inside the collecting system.\\nPatient: What did the microscopic examination of the tumor show?\\nDoctor: The microscopic examination showed mature fat tissue devoid of normal architecture, owing to expanded interstitial spaces either because of intense edema or irregular fibrous streaks. Fibrous septa between fat lobules contained mildly increased numbers of stromal fibroblasts, fine collagen, vascular proliferation, multifocal lymphocytic infiltration, occasional lymphoid follicles, and foamy histiocytes. Plasma cells were rare, and immunohistochemistry showed only a <2% IgG4+/ IgG+ plasma cell ratio. There were also scattered smooth muscle bundles, usually in close association with vessels. Ectatic branching lymphatic channels were not noted, and there were neither lipoblasts nor significant cellular atypia. A few scattered fibroblasts carried multilobulated large nuclei.\\nPatient: So, what does all of this mean for me?\\nDoctor: Your results indicate that the tumor has been successfully removed, and there are no signs of aggressive cancer. However, it is essential to monitor your condition and attend regular follow-up appointments to ensure everything remains stable.\\nPatient: Thank you, doctor. I'll make sure to schedule those follow-up appointments and keep an eye on my health.\\nDoctor: You're welcome. If you have any concerns or notice any changes in your health, please don't hesitate to contact me or schedule an appointment. Take care!\"}, {'role': 'assistant', 'content': \"S: The patient reported undergoing an abdominal ultrasonography for bilateral renal nephrolithiasis. He has a history of extrophic bladder repair with an Indiana pouch created 17 years ago. He denies any cardiovascular or hormonal disorders. His chief complaint relates to findings from recent imaging studies indicating a significant renal issue.\\nO: The patient's weight is 85 kg, and his BMI is 28.7 kg/m2. Blood biochemistry is within normal limits. Abdominal ultrasonography and CT scan revealed a hyperechogenic lesion at the fat intensity in the right renal sinus, extending to the pelvis and cuffing the right ureter, causing severe hydronephrosis. The left kidney shows a small scar but is otherwise unremarkable. A right radical nephroureterectomy was performed, removing a 16x13x6 cm fatty mass with features suggestive of liposarcoma. Microscopic examination showed mature fat tissue with expanded interstitial spaces, mild stromal fibroblast increase, and no significant cellular atypia or lipoblasts.\\nA: The primary diagnosis is a benign fatty tumor of the right kidney with severe hydronephrosis and renal atrophy, likely secondary to the mass effect and chronic obstruction. Differential diagnosis included liposarcoma, which was ruled out based on histopathological findings. The prognosis is good following surgical resection, but regular monitoring is necessary to detect any recurrence or new complications.\\nP: The patient is advised to schedule regular follow-up appointments for monitoring his renal function and to detect any signs of recurrence early. He should maintain a healthy diet and hydration to support kidney function. Any new symptoms or changes in health should prompt an immediate consultation. Further consultations with a nephrologist and a urologist may be necessary to manage long-term outcomes and monitor renal function.\"}]}\n"
     ]
    }
   ],
   "source": [
    "print(f'train set size: {len(train_data)}')\n",
    "print(f'valid set size: {len(valid_data)}')\n",
    "print(f'test set size: {len(test_data)}')\n",
    "print(next(iter(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae6e66ca-0c96-4e0f-8345-bbe46ef166fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"santiviquez/ssr-base-finetuned-samsum-en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bdca55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[7582,   10,  363,  ...,    0,    0,    0]])\n",
      "Tokens: ['▁Doctor', ':', '▁What', '▁brings', '▁you', '▁back', '▁into', '▁the', '▁clinic', '▁today', ',', '▁miss', '?', '▁Patient', ':', '▁I', '▁came', '▁in', '▁for', '▁', 'a', '▁refill', '▁of', '▁my', '▁blood', '▁pressure', '▁medicine', '.', '▁Doctor', ':', '▁It', '▁looks', '▁like', '▁Doctor', '▁Kumar', '▁followed', '▁up', '▁with', '▁you', '▁last', '▁time', '▁regarding', '▁your', '▁hyper', 'tension', ',', '▁osteo', 'arth', 'riti', 's', ',', '▁osteo', 'p', 'o', 'ros', 'is', ',', '▁hypo', 't', 'hyroid', 'is', 'm', ',', '▁allergic', '▁', 'r', 'hin', 'it', 'is', '▁and', '▁kidney', '▁stones', '.', '▁Have', '▁you', '▁noticed', '▁any', '▁changes', '▁or', '▁do', '▁you', '▁have', '▁any', '▁concerns', '▁regarding', '▁these', '▁issues', '?', '▁Patient', ':', '▁No', '.', '▁Doctor', ':', '▁Have', '▁you', '▁had', '▁any', '▁fever', '▁or', '▁chill', 's', ',', '▁cough', ',', '▁congestion', ',', '▁nausea', ',', '▁vomiting', ',', '▁chest', '▁pain', ',', '▁chest', '▁pressure', '?', 'P', 'a', 'tient', ':', '▁No', '.', '▁Doctor', ':', '▁Great', '.', '▁Also', ',', '▁for', '▁our', '▁records', ',', '▁how', '▁old', '▁are', '▁you', '▁and', '▁what', '▁race', '▁do', '▁you', '▁identify', '▁yourself', '▁as', '?', 'P', 'a', 'tient', ':', '▁I', '▁am', '▁seven', 't', 'y', '▁six', '▁years', '▁old', '▁and', '▁identify', '▁as', '▁', 'a', '▁white', '▁female', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "dialogue = \"Doctor: What brings you back into the clinic today, miss? Patient: I came in for a refill of my blood pressure medicine. Doctor: It looks like Doctor Kumar followed up with you last time regarding your hypertension, osteoarthritis, osteoporosis, hypothyroidism, allergic rhinitis and kidney stones.  Have you noticed any changes or do you have any concerns regarding these issues? Patient: No. Doctor: Have you had any fever or chills, cough, congestion, nausea, vomiting, chest pain, chest pressure?Patient: No. Doctor: Great. Also, for our records, how old are you and what race do you identify yourself as?Patient: I am seventy six years old and identify as a white female.\"\n",
    "inputs = tokenizer(dialogue, return_tensors=\"pt\", max_length=1024, truncation=True, padding=\"max_length\")\n",
    "summary = \"The patient is a 76-year-old white female who presents to the clinic today originally for hypertension and a med check.  She has a history of hypertension, osteoarthritis, osteoporosis, hypothyroidism, allergic rhinitis and kidney stones.  Since her last visit she has been followed by Dr. Kumar.  Those issues are stable.  She has had no fever or chills, cough, congestion, nausea, vomiting, chest pain, chest pressure.\"\n",
    "# 对目标摘要进行编码\n",
    "targets = tokenizer(summary, return_tensors=\"pt\", max_length=1024, truncation=True, padding=\"max_length\")\n",
    "# 打印输入的令牌ID和对应的文本表示\n",
    "print('Token IDs:', inputs['input_ids'])\n",
    "print('Tokens:', tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05921991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d1a2e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jie/.local/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSeq2SeqLM, AdamW\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 256\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"santiviquez/ssr-base-finetuned-samsum-en\").to(device).half()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "109b80ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cpu\")  # 显式设置为 CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "523d179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collote_fn(batch_samples):\n",
    "    batch_inputs, batch_targets = [], []\n",
    "    for sample in batch_samples:\n",
    "        batch_inputs.append(sample['dialogue'])\n",
    "        batch_targets.append(sample['summary'])\n",
    "    batch_data = tokenizer(\n",
    "        batch_inputs, \n",
    "        padding=True, \n",
    "        max_length=max_input_length,\n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch_targets, \n",
    "            padding=True, \n",
    "            max_length=max_target_length,\n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "        batch_data['decoder_input_ids'] = model.prepare_decoder_input_ids_from_labels(labels)\n",
    "        end_token_index = torch.where(labels == tokenizer.eos_token_id)[1]\n",
    "        for idx, end_idx in enumerate(end_token_index):\n",
    "            labels[idx][end_idx+1:] = -100\n",
    "        batch_data['labels'] = labels\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a4c2710",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=collote_fn)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=4, shuffle=False, collate_fn=collote_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=4,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ab7be19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Summaries: 100%|██████████| 63/63 [01:16<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores: {'rouge1': 0.10834107551731281, 'rouge2': 0.05164251028567105, 'rougeL': 0.08681422112581574}\n",
      "Pre-training predictions and ROUGE scores saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# 初始化 ROUGE 评分器\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "def predict_summary(dialogue, model, tokenizer):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(dialogue, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(device)\n",
    "        outputs = model.generate(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=512)\n",
    "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "results = []\n",
    "rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "for batch in tqdm(test_dataloader, desc=\"Generating Summaries\"):\n",
    "    dialogues = batch['dialogue']\n",
    "    reference_summaries = batch['summary']\n",
    "    \n",
    "    for dialogue, reference_summary in zip(dialogues, reference_summaries):\n",
    "        predicted_summary = predict_summary(dialogue, model, tokenizer)\n",
    "        scores = scorer.score(reference_summary, predicted_summary)\n",
    "        results.append({\n",
    "            \"Dialogue\": dialogue,\n",
    "            \"Reference Summary\": reference_summary,\n",
    "            \"Predicted Summary\": predicted_summary,\n",
    "            \"ROUGE-1\": scores['rouge1'].fmeasure,\n",
    "            \"ROUGE-2\": scores['rouge2'].fmeasure,\n",
    "            \"ROUGE-L\": scores['rougeL'].fmeasure\n",
    "        })\n",
    "\n",
    "        # Accumulate scores\n",
    "        rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "        rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "        rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "# 计算平均 ROUGE 分数\n",
    "average_scores = {key: sum(values) / len(values) for key, values in rouge_scores.items()}\n",
    "print(\"Average ROUGE Scores:\", average_scores)\n",
    "\n",
    "# 保存到 JSON 文件\n",
    "with open('pre_ssr_base_finetuned_samsum_rouge_scores.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Pre-training predictions and ROUGE scores saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ebca85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MedicalDialogueDataset(Dataset):\n",
    "    def __init__(self, split, percent=100, seed=42):  # 添加seed参数，默认值为42\n",
    "        # 加载数据集\n",
    "        ds = load_dataset(\"omi-health/medical-dialogue-to-soap-summary\", split=split)\n",
    "        \n",
    "        # 移除不需要的列\n",
    "        columns_to_remove = ['messages', 'prompt']\n",
    "        ds = ds.remove_columns(columns_to_remove)\n",
    "        \n",
    "        # 替换换行符并重命名列\n",
    "        ds = ds.rename_column('soap', 'summary')\n",
    "        \n",
    "        # 添加ID和格式化摘要\n",
    "        ds = ds.map(self.add_id, with_indices=True)\n",
    "        ds = ds.map(self.format_summary)\n",
    "        \n",
    "        # 如果需要子集，先随机打乱，再选择对应的百分比\n",
    "        if percent < 100:\n",
    "            ds = ds.shuffle(seed=seed).select(range(int(percent / 100.0 * len(ds))))\n",
    "        \n",
    "        self.data = ds\n",
    "    \n",
    "    def add_id(self, example, idx):\n",
    "        example['id'] = str(idx)\n",
    "        return example\n",
    "    \n",
    "    def format_summary(self, example):\n",
    "        example['summary'] = example['summary'].replace('S: ', 'Subjective: ')\n",
    "        example['summary'] = example['summary'].replace('O: ', 'Objective: ')\n",
    "        example['summary'] = example['summary'].replace('A: ', 'Assessment: ')\n",
    "        example['summary'] = example['summary'].replace('P: ', 'Plan: ')\n",
    "        return example\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]  # 获取索引对应的数据项\n",
    "        ordered_item = {'id': item['id']}  # 创建一个新字典，并首先加入'id'\n",
    "        ordered_item.update({k: item[k] for k in item if k != 'id'})  # 添加其他字段，排除'id'\n",
    "        return item['dialogue'], item['summary'] \n",
    "        # return ordered_item\n",
    "\n",
    "# 创建不同百分比的训练数据集实例\n",
    "# train_data = MedicalDialogueDataset('train', percent=20, seed=42)\n",
    "train_data = MedicalDialogueDataset('train', percent=50, seed=42)\n",
    "# train_data = MedicalDialogueDataset('train', percent=100, seed=42)  \n",
    "\n",
    "valid_data = MedicalDialogueDataset('validation')\n",
    "test_data = MedicalDialogueDataset('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2608252",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "model_name = 'pre_ssr_base_finetuned_samsum'\n",
    "\n",
    "results = []\n",
    "for idx in range(len(test_data)):\n",
    "    try:\n",
    "        dialogue, reference_summary = test_data[idx]\n",
    "        predicted_summary = predict_summary(dialogue, model, tokenizer)\n",
    "        results.append({\n",
    "            \"Dialogue\": dialogue,\n",
    "            \"Reference Summary\": reference_summary,\n",
    "            \"Predicted Summary\": predicted_summary\n",
    "        })\n",
    "    except ValueError as e:\n",
    "        print(f\"Error at index {idx}: {e}\")\n",
    "\n",
    "\n",
    "# 保存到CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(f\"{model_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cb4d075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MedicalDialogueDataset(Dataset):\n",
    "    def __init__(self, split, percent=100, seed=42):  # 添加seed参数，默认值为42\n",
    "        # 加载数据集\n",
    "        ds = load_dataset(\"omi-health/medical-dialogue-to-soap-summary\", split=split)\n",
    "        \n",
    "        # 移除不需要的列\n",
    "        columns_to_remove = ['messages', 'prompt']\n",
    "        ds = ds.remove_columns(columns_to_remove)\n",
    "        \n",
    "        # 替换换行符并重命名列\n",
    "        ds = ds.rename_column('soap', 'summary')\n",
    "        \n",
    "        # 添加ID和格式化摘要\n",
    "        ds = ds.map(self.add_id, with_indices=True)\n",
    "        ds = ds.map(self.format_summary)\n",
    "        \n",
    "        # 如果需要子集，先随机打乱，再选择对应的百分比\n",
    "        if percent < 100:\n",
    "            ds = ds.shuffle(seed=seed).select(range(int(percent / 100.0 * len(ds))))\n",
    "        \n",
    "        self.data = ds\n",
    "    \n",
    "    def add_id(self, example, idx):\n",
    "        example['id'] = str(idx)\n",
    "        return example\n",
    "    \n",
    "    def format_summary(self, example):\n",
    "        example['summary'] = example['summary'].replace('S: ', 'Subjective: ')\n",
    "        example['summary'] = example['summary'].replace('O: ', 'Objective: ')\n",
    "        example['summary'] = example['summary'].replace('A: ', 'Assessment: ')\n",
    "        example['summary'] = example['summary'].replace('P: ', 'Plan: ')\n",
    "        return example\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]  # 获取索引对应的数据项\n",
    "        ordered_item = {'id': item['id']}  # 创建一个新字典，并首先加入'id'\n",
    "        ordered_item.update({k: item[k] for k in item if k != 'id'})  # 添加其他字段，排除'id'\n",
    "        # return item['dialogue'], item['summary'] \n",
    "        return ordered_item\n",
    "\n",
    "# 创建不同百分比的训练数据集实例\n",
    "# train_data = MedicalDialogueDataset('train', percent=20, seed=42)\n",
    "train_data = MedicalDialogueDataset('train', percent=50, seed=42)\n",
    "# train_data = MedicalDialogueDataset('train', percent=100, seed=42)  \n",
    "\n",
    "valid_data = MedicalDialogueDataset('validation')\n",
    "test_data = MedicalDialogueDataset('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6300f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'decoder_input_ids', 'labels'])\n",
      "batch shape: {'input_ids': torch.Size([4, 512]), 'attention_mask': torch.Size([4, 512]), 'decoder_input_ids': torch.Size([4, 256]), 'labels': torch.Size([4, 256])}\n",
      "{'input_ids': tensor([[ 7582,    10,  2018,  ...,  8257,   308,     1],\n",
      "        [ 7582,    10,  8774,  ...,     6,    34,     1],\n",
      "        [ 7582,    10,  1804,  ...,  1181,  9940,     1],\n",
      "        [ 7582,    10,  8774,  ..., 20917,   116,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'decoder_input_ids': tensor([[    0, 19237,   757,  ...,   743,     3,  1913],\n",
      "        [    0, 19237,   757,  ...,   411,     7,    15],\n",
      "        [    0, 19237,   757,  ...,     9,  1109,     9],\n",
      "        [    0, 19237,   757,  ...,    28,     3,     9]]), 'labels': tensor([[19237,   757,    10,  ...,     3,  1913,     1],\n",
      "        [19237,   757,    10,  ...,     7,    15,     1],\n",
      "        [19237,   757,    10,  ...,  1109,     9,     1],\n",
      "        [19237,   757,    10,  ...,     3,     9,     1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jie/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:4016: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())\n",
    "print('batch shape:', {k: v.shape for k, v in batch.items()})\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49079eda-9572-48a6-8f20-b162e0026f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_batch_num = (epoch-1) * len(dataloader)\n",
    "    \n",
    "    model.train()\n",
    "    for batch, batch_data in enumerate(dataloader, start=1):\n",
    "        batch_data = batch_data.to(device)\n",
    "        outputs = model(**batch_data)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {total_loss/(finish_batch_num + batch):>7f}')\n",
    "        progress_bar.update(1)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fc8be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "877d2b03-5c73-464a-851e-941e09121994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 1.0, 'p': 0.8571428571428571, 'f': 0.9230769181065088}, 'rouge-2': {'r': 0.8, 'p': 0.6666666666666666, 'f': 0.7272727223140496}, 'rouge-l': {'r': 1.0, 'p': 0.8571428571428571, 'f': 0.9230769181065088}}\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "generated_summary = \"I absolutely loved reading the Hunger Games\"\n",
    "reference_summary = \"I loved reading the Hunger Games\"\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "scores = rouge.get_scores(\n",
    "    hyps=[generated_summary], refs=[reference_summary]\n",
    ")[0]\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "640a2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "def test_loop(dataloader, model):\n",
    "\n",
    "    preds, labels = [], []\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_data in tqdm(dataloader):\n",
    "        # batch_data = batch_data.to(device)\n",
    "        batch_data = {k: v.to(device) for k, v in batch_data.items()}\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                batch_data[\"input_ids\"],\n",
    "                attention_mask=batch_data[\"attention_mask\"],\n",
    "                max_length=max_target_length,\n",
    "                num_beams=4,\n",
    "                no_repeat_ngram_size=2,\n",
    "            ).cpu().numpy()\n",
    "        if isinstance(generated_tokens, tuple):\n",
    "            generated_tokens = generated_tokens[0]\n",
    "        label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "        preds += [' '.join(pred.strip()) for pred in decoded_preds]\n",
    "        labels += [' '.join(label.strip()) for label in decoded_labels]\n",
    "    scores = rouge.get_scores(hyps=preds, refs=labels, avg=True)\n",
    "    result = {key: value['f'] * 100 for key, value in scores.items()}\n",
    "    result['avg'] = np.mean(list(result.values()))\n",
    "    print(f\"Rouge1: {result['rouge-1']:>0.2f} Rouge2: {result['rouge-2']:>0.2f} RougeL: {result['rouge-l']:>0.2f}\\n\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fc7af19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.436058: 100%|██████████| 1157/1157 [04:30<00:00,  4.28it/s]\n",
      "100%|██████████| 125/125 [07:04<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge1: 90.35 Rouge2: 74.28 RougeL: 86.18\n",
      "\n",
      "{'rouge-1': 90.34601830615604, 'rouge-2': 74.28039302180336, 'rouge-l': 86.18447287945835, 'avg': 83.60362806913925}\n",
      "saving new weights...\n",
      "\n",
      "Epoch 2/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.110894: 100%|██████████| 1157/1157 [04:17<00:00,  4.49it/s]\n",
      "100%|██████████| 125/125 [07:22<00:00,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge1: 90.88 Rouge2: 74.85 RougeL: 86.95\n",
      "\n",
      "{'rouge-1': 90.87632308901225, 'rouge-2': 74.85426928781551, 'rouge-l': 86.95288697845413, 'avg': 84.22782645176063}\n",
      "saving new weights...\n",
      "\n",
      "Epoch 3/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.988862: 100%|██████████| 1157/1157 [04:16<00:00,  4.51it/s]\n",
      "100%|██████████| 125/125 [07:20<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge1: 91.47 Rouge2: 75.23 RougeL: 87.84\n",
      "\n",
      "{'rouge-1': 91.46595772836594, 'rouge-2': 75.22683993950103, 'rouge-l': 87.84455601696008, 'avg': 84.84578456160902}\n",
      "saving new weights...\n",
      "\n",
      "Epoch 4/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.923790: 100%|██████████| 1157/1157 [04:18<00:00,  4.47it/s]\n",
      "100%|██████████| 125/125 [06:46<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge1: 91.48 Rouge2: 75.25 RougeL: 87.72\n",
      "\n",
      "{'rouge-1': 91.48110530763238, 'rouge-2': 75.24751530159472, 'rouge-l': 87.7163765397374, 'avg': 84.81499904965483}\n",
      "Epoch 5/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.883558: 100%|██████████| 1157/1157 [04:15<00:00,  4.52it/s]\n",
      "100%|██████████| 125/125 [06:46<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge1: 91.73 Rouge2: 75.40 RougeL: 88.17\n",
      "\n",
      "{'rouge-1': 91.7331399105545, 'rouge-2': 75.40030584894669, 'rouge-l': 88.1749916525887, 'avg': 85.10281247069662}\n",
      "saving new weights...\n",
      "\n",
      "Epoch 6/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.856027: 100%|██████████| 1157/1157 [04:17<00:00,  4.50it/s]\n",
      "100%|██████████| 125/125 [06:57<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge1: 91.62 Rouge2: 75.15 RougeL: 87.99\n",
      "\n",
      "{'rouge-1': 91.62191048636588, 'rouge-2': 75.14668055085558, 'rouge-l': 87.98652893080542, 'avg': 84.91837332267562}\n",
      "Epoch 7/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.836139: 100%|██████████| 1157/1157 [04:34<00:00,  4.21it/s]\n",
      "100%|██████████| 125/125 [07:34<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge1: 91.58 Rouge2: 75.37 RougeL: 88.00\n",
      "\n",
      "{'rouge-1': 91.58232764400522, 'rouge-2': 75.36646533730521, 'rouge-l': 88.00322810393858, 'avg': 84.98400702841633}\n",
      "Epoch 8/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.820832: 100%|██████████| 1157/1157 [04:16<00:00,  4.52it/s]\n",
      "100%|██████████| 125/125 [06:48<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge1: 91.78 Rouge2: 75.28 RougeL: 88.03\n",
      "\n",
      "{'rouge-1': 91.77953450990563, 'rouge-2': 75.2782206300788, 'rouge-l': 88.0297875494094, 'avg': 85.02918089646461}\n",
      "Epoch 9/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.808993: 100%|██████████| 1157/1157 [04:33<00:00,  4.23it/s]\n",
      "100%|██████████| 125/125 [07:10<00:00,  3.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge1: 91.61 Rouge2: 75.30 RougeL: 88.02\n",
      "\n",
      "{'rouge-1': 91.61390305518094, 'rouge-2': 75.29577690304883, 'rouge-l': 88.01560631100857, 'avg': 84.97509542307945}\n",
      "Epoch 10/10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.799514: 100%|██████████| 1157/1157 [04:15<00:00,  4.53it/s]\n",
      "100%|██████████| 125/125 [06:47<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge1: 91.61 Rouge2: 75.31 RougeL: 87.98\n",
      "\n",
      "{'rouge-1': 91.60809422435275, 'rouge-2': 75.31408257347202, 'rouge-l': 87.9842837682811, 'avg': 84.96882018870195}\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "learning_rate = 1e-7\n",
    "epoch_num = 10\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num*len(train_dataloader),\n",
    ")\n",
    "\n",
    "total_loss = 0.\n",
    "best_avg_rouge = 0.\n",
    "for t in range(epoch_num):\n",
    "    print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, optimizer, lr_scheduler, t+1, total_loss)\n",
    "    valid_rouge = test_loop(valid_dataloader, model)\n",
    "    print(valid_rouge)\n",
    "    rouge_avg = valid_rouge['avg']\n",
    "    if rouge_avg > best_avg_rouge:\n",
    "        best_avg_rouge = rouge_avg\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(model.state_dict(), f'epoch_{t+1}_valid_rouge_{rouge_avg:0.4f}_model_weights.bin')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40026d51",
   "metadata": {},
   "source": [
    "## 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98b9fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_data, batch_size = 4, shuffle = False, collate_fn = collote_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb5bf5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('epoch_5_valid_rouge_85.1028_model_weights.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea6b6ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [03:24<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Rouge1: 90.92 Rouge2: 75.36 RougeL: 87.21\n",
      "\n",
      "saving predicted results...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('evaluating on test set...')\n",
    "    sources, preds, labels = [], [], []\n",
    "    for batch_data in tqdm(test_dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        generated_tokens = model.generate(\n",
    "            batch_data[\"input_ids\"],\n",
    "            attention_mask=batch_data[\"attention_mask\"],\n",
    "            max_length=max_target_length,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=2,\n",
    "        )\n",
    "\n",
    "        generated_tokens = generated_tokens.cpu().numpy()\n",
    "        label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "\n",
    "        decoded_sources = tokenizer.batch_decode(\n",
    "            batch_data[\"input_ids\"].cpu().numpy(), \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "        sources += [source.strip() for source in decoded_sources]\n",
    "        preds += [pred.strip() for pred in decoded_preds]\n",
    "        labels += [label.strip() for label in decoded_labels]\n",
    "    scores = rouge.get_scores(\n",
    "        hyps=[' '.join(pred) for pred in preds], \n",
    "        refs=[' '.join(label) for label in labels], \n",
    "        avg=True\n",
    "    )\n",
    "    rouges = {key: value['f'] * 100 for key, value in scores.items()}\n",
    "    rouges['avg'] = np.mean(list(rouges.values()))\n",
    "    print(f\"Test Rouge1: {rouges['rouge-1']:>0.2f} Rouge2: {rouges['rouge-2']:>0.2f} RougeL: {rouges['rouge-l']:>0.2f}\\n\")\n",
    "    results = []\n",
    "    print('saving predicted results...')\n",
    "    for source, pred, label in zip(sources, preds, labels):\n",
    "        results.append({\n",
    "            \"document\": source, \n",
    "            \"prediction\": pred, \n",
    "            \"summarization\": label\n",
    "        })\n",
    "    with open('post_ssr_base_test_data_pred.json', 'wt', encoding='utf-8') as f:\n",
    "        for exapmle_result in results:\n",
    "            f.write(json.dumps(exapmle_result, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9436b1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjective: The patient had an emergency caesarean delivery at 39 weeks due to fetal distress. She had a saline contrast sonohysterography 6 months after the birth, and the remaining myometrium over the defect was 7.5 mm (Fig.) Despite no discomfort or symptoms, her current pregnancy developed normally with no signs of abnormal placentation or ectopic gestational sac was not visualized with transvaginal or transabdominal scans at 18, 22 and 30 weeks of pregnancy. The ultrasound examination revealed the duplex pregnancy with one viable intrauterine fetus with normal anatomy and Placenta located high on the anterior wall and no extensive vascularity surrounding the sac without an embryo. There was no indications of contraindications for vaginaal delivery, the thickness of the lower uterin segment (LUS) was unchanged at all visits.\n"
     ]
    }
   ],
   "source": [
    "def predict_summary(input_text, model, tokenizer, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True, padding=\"max_length\")\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=512,\n",
    "        num_beams=10,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=False\n",
    "    )\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    return summary\n",
    "\n",
    "# 使用函数\n",
    "input_text = \"Doctor: Hello, I remember you had an emergency caesarean delivery at 39 weeks due to fetal distress. How have you been since then? Any postpartum complications? Patient: Hi, Doctor. I've been doing well since the delivery. No complications, thankfully. Doctor: That's good to hear. As part of our ongoing study on 'Vaginal delivery after caesarean section', you underwent a saline contrast sonohysterography 6 months after the caesarean section. The results showed a small indentation in your caesarean scar, and the remaining myometrium over the defect was 7.5 mm (Fig. ). Patient: Oh, I see. What does that mean for my current pregnancy? Doctor: At around 11 weeks, you had a dating scan with no remarks. Then, you came for a transvaginal ultrasound examination at around 13 weeks asc part of our study. The scan revealed a duplex pregnancy with one viable intrauterine fetus with normal anatomy and placenta located high on the anterior wall. A small gestational sac (8 mm) with a yolk sac without an embryo was located in the caesarean scar (Fig. ). There was no extensive vascularity surrounding the sac, and you were asymptomatic. Patient: Yes, that's right. I didn't feel any discomfort or symptoms. Doctor: We informed you that there wasn't enough evidence to advise a specific management for this condition. After discussion with you and your husband, expectant management was chosen with a new ultrasound examination scheduled after 5 weeks. Patient: Yes, we decided to wait and see how things would progress. Doctor: You came to our ultrasound department at 18 weeks, 22 weeks, and 30 weeks of gestation. Throughout this time, you remained asymptomatic. The ectopic gestational sac was not visualized with transvaginal or transabdominal scans at the 18 weeks examination (Fig. ). The niche in the scar and the thickness of the thinnest part of the remaining myometrium appeared unchanged at all visits. Patient: That's a relief. How's the intrauterine pregnancy developing? Doctor: The intrauterine pregnancy developed normally with no signs of abnormal placentation. At 30 weeks of gestation, the ultrasound appearance of the scar area did not indicate any contraindications for vaginal delivery. The thickness of the lower uterine segment (LUS) was 4.9 mm (Fig. ). Patient: So, I can have a vaginal delivery this time? Doctor: Yes, in agreement with you, we've planned for a vaginal delivery. The staff of the labor ward has been fully informed and prepared for your case. Patient: That's great news! Thank you, Doctor. Doctor: You're welcome. You'll be admitted to the labor ward when the time comes. Please continue to monitor your symptoms and reach out if you have any concerns. Good luck with the rest of your pregnancy. Patient: Thank you so much, Doctor. I appreciate your help and guidance throughout this process.\"\n",
    "summary = predict_summary(input_text, model, tokenizer, device)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfbb6824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MedicalDialogueDataset(Dataset):\n",
    "    def __init__(self, split, percent=100, seed=42):  # 添加seed参数，默认值为42\n",
    "        # 加载数据集\n",
    "        ds = load_dataset(\"omi-health/medical-dialogue-to-soap-summary\", split=split)\n",
    "        \n",
    "        # 移除不需要的列\n",
    "        columns_to_remove = ['messages', 'prompt']\n",
    "        ds = ds.remove_columns(columns_to_remove)\n",
    "        \n",
    "        # 替换换行符并重命名列\n",
    "        ds = ds.rename_column('soap', 'summary')\n",
    "        \n",
    "        # 添加ID和格式化摘要\n",
    "        ds = ds.map(self.add_id, with_indices=True)\n",
    "        ds = ds.map(self.format_summary)\n",
    "        \n",
    "        # 如果需要子集，先随机打乱，再选择对应的百分比\n",
    "        if percent < 100:\n",
    "            ds = ds.shuffle(seed=seed).select(range(int(percent / 100.0 * len(ds))))\n",
    "        \n",
    "        self.data = ds\n",
    "    \n",
    "    def add_id(self, example, idx):\n",
    "        example['id'] = str(idx)\n",
    "        return example\n",
    "    \n",
    "    def format_summary(self, example):\n",
    "        example['summary'] = example['summary'].replace('S: ', 'Subjective: ')\n",
    "        example['summary'] = example['summary'].replace('O: ', 'Objective: ')\n",
    "        example['summary'] = example['summary'].replace('A: ', 'Assessment: ')\n",
    "        example['summary'] = example['summary'].replace('P: ', 'Plan: ')\n",
    "        return example\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]  # 获取索引对应的数据项\n",
    "        ordered_item = {'id': item['id']}  # 创建一个新字典，并首先加入'id'\n",
    "        ordered_item.update({k: item[k] for k in item if k != 'id'})  # 添加其他字段，排除'id'\n",
    "        return item['dialogue'], item['summary'] \n",
    "        # return ordered_item\n",
    "\n",
    "# 创建不同百分比的训练数据集实例\n",
    "# train_data = MedicalDialogueDataset('train', percent=20, seed=42)\n",
    "train_data = MedicalDialogueDataset('train', percent=50, seed=42)\n",
    "# train_data = MedicalDialogueDataset('train', percent=100, seed=42)  \n",
    "\n",
    "valid_data = MedicalDialogueDataset('validation')\n",
    "test_data = MedicalDialogueDataset('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d91db491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "model_name = 'ssr-base-finetuned-samsum-en'\n",
    "results = []\n",
    "for idx in range(len(test_data)):  # 遍历整个测试集\n",
    "    dialogue, reference_summary = test_data[idx]\n",
    "    predicted_summary = predict_summary(dialogue, model, tokenizer)\n",
    "    results.append({\n",
    "        \"Dialogue\": dialogue,\n",
    "        \"Reference Summary\": reference_summary,\n",
    "        \"Predicted Summary\": predicted_summary\n",
    "    })\n",
    "\n",
    "# 保存到CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(f\"post_{model_name}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
